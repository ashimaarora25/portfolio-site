<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>My Data Engineering Portfolio</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Hi, I'm Ashima Arora</h1>
    <p>Data Engineer | Transitioning into ML/MLOps</p>
  </header>

  <section id="projects">
    <h2>Featured Projects</h2>
    <div class="project-card">
      <h3> Daily Automated ETL Pipeline with Apache Airflow & PostgreSQL </h3>
      <p> Designed and deployed a daily automated ETL pipeline that leverages Apache Airflow to orchestrate the ingestion and processing of Amazon book metadata. Python scripts handle data transformation before loading it into a PostgreSQL database. This setup fully automates the data ingestion, transformation, and storage process, enabling efficient management and querying of structured Amazon books data.</p>
      <img src = "pipeline_design-2.png" width="750" height="430"> <br>
      <a href="https://github.com/ashimaarora25/bigdataprojects/tree/main/amazonbooksdata" target="_blank">View on GitHub</a>
    </div>
    <div class="project-card">
      <h3> Real-Time ETL Pipeline with Apache Kafka & AWS </h3>
      <p> Designed and deployed an end-to-end ETL pipeline to simulate near real-time streaming data generation using Python. Implemented Apache Kafka for data ingestion and processing, with Kafka Producers and Consumers publishing to and consuming from a Kafka Broker hosted on Amazon EC2. Processed data is stored in Amazon S3, where an AWS Glue Crawler catalogs the data. Using Amazon Athena, the cataloged data is queried and loaded into a SQL database for downstream analytics. This setup enables continuous, low-latency data ingestion, transformation, and analytics, supporting near real-time insights on stock market data.  </p>
      <img src = "Architecture.jpg" alt ="Data pipeline for ingestion of streaming stock market data" width="750" height="430"> <br>
      <a href="https://github.com/ashimaarora25/bigdataprojects/tree/main/kafka-stockmarket" target="_blank">View on GitHub</a>
    </div>
  </section>

  <section id="about">
    <h2>About Me</h2>
    <p>I’m a Data Engineer with a Master’s degree in Computer Science, specializing in Machine Learning. My expertise lies in building and optimizing scalable data pipelines, integrating diverse data sources, and enabling analytics-ready datasets using big data technologies like Hadoop, Hive, and Spark (PySpark).

      In my current role, I’ve led initiatives that improved data accessibility by 60%, reduced pipeline-related incidents by 50%, and boosted operational efficiency by 40% through automation and optimization. My experience spans both batch and near real-time architectures, cloud-based data solutions, and the use of Apache Kafka for high-throughput streaming.

      With my academic background in ML, I’m now expanding into Machine Learning Engineering and MLOps — leveraging my data engineering foundation to develop, deploy, and maintain intelligent systems at scale. This portfolio is a curated collection of projects that demonstrate how I apply theory to practice, bridging the gap between raw data and impactful, data-driven solutions.</p>
  </section>

  <section id="contact">
    <h2>Contact</h2>
    <p>Email: ashimaarora63@gmail.com</p>
    <p>LinkedIn: <a href="https://www.linkedin.com/in/ashima-arora-" target="_blank">Ashima Arora</a></p>
  </section>

  <footer>
    <p>&copy; 2025 Ashima Arora</p>
  </footer>
</body>
</html>
